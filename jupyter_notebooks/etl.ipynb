{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **ETL (Extract, Transform, Load)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "- Load the raw **VGChartz Video Game Sales** dataset and prepare it for analysis and dashboarding.  \n",
        "- Perform basic data profiling to understand structure and quality.  \n",
        "- Clean and transform the dataset (handle missing values, unify formats, engineer features such as multi-platform indicator, first-party flag, and release era).  \n",
        "- Export a cleaned, analysis-ready dataset for use in visualizations and Tableau.\n",
        "\n",
        "## Inputs\n",
        "- **Raw data file:** `data/raw/Video_Games_Sales_as_at_22_Dec_2016.csv`  \n",
        "- **Columns used:**  \n",
        "  `Name`, `Platform`, `Year_of_Release`, `Genre`, `Publisher`,  \n",
        "  `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Global_Sales`,  \n",
        "  `Critic_Score`, `Critic_Count`, `User_Score`, `User_Count`, `Developer`, `Rating`  \n",
        "- **Python libraries:** `pandas`, `numpy`, `matplotlib`, `seaborn` (for quick profiling)\n",
        "\n",
        "## Outputs\n",
        "- **Processed dataset:** `data/processed/video_game_sales_clean.csv` — cleaned and feature-engineered for analysis.  \n",
        "- Summary of data issues and cleaning actions in the ETL notebook (`notebooks/etl.ipynb`).  \n",
        "- Basic exploratory statistics (row counts, missing values, data types) for reference.\n",
        "\n",
        "## Additional Comments\n",
        "- Major cleaning steps include:  \n",
        "  - Removing rows with no game name or no sales data.  \n",
        "  - Converting year to integer and handling missing or unrealistic years.  \n",
        "  - Dropping or flagging games without review scores when needed for hypotheses.  \n",
        "  - Creating new features:  \n",
        "    - `Vendor` (Nintendo, Sony, Microsoft, Other)  \n",
        "    - `is_multiplatform` (1 if game appears on ≥2 platforms)  \n",
        "    - `is_first_party` (1 if publisher matches platform vendor)  \n",
        "    - `Era` (pre-2010 vs post-2010 for trend analysis)  \n",
        "- This notebook produces the single source of truth dataset used throughout the project (analysis, testing, and Tableau dashboard).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the data directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the file path for the raw data\n",
        "raw_data_dir = os.path.join(current_dir, 'data/raw')\n",
        "\n",
        "# Set the file path for the processed data\n",
        "processed_data_dir = os.path.join(current_dir, 'data/processed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Raw data directory:\", raw_data_dir)\n",
        "print(\"Processed data directory:\", processed_data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports\n",
        "\n",
        "Import the necessary packages to perform the ETL process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from utils.cleaning import remove_review_cols, convert_dtypes, clean_data\n",
        "from utils.game_merger import build_merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(raw_data_dir, 'video_game_sales.csv'))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Profiling\n",
        "\n",
        "Understanding the structure and basic info of the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This dataset contains 16719 rows and 16 columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check and convert datatypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Type Adjustments\n",
        "\n",
        "To prepare the dataset for analysis, several columns were converted to more suitable data types:\n",
        "\n",
        "- **Year_of_Release** → changed from `float64` to `Int64` (nullable integer) to store whole years and handle missing values.\n",
        "- **Critic_Score** → optionally converted to `Int64` since scores are whole numbers.\n",
        "- **Platform, Genre, Publisher, Developer, Rating** → converted from `object` to `category` to reduce memory use and speed up grouping/filtering.\n",
        "\n",
        "These changes make the dataset cleaner, improve performance, and prevent issues when running statistical tests or creating visualisations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = convert_dtypes(df)\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key observations:**\n",
        "\n",
        "- **Sales data** (`NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Global_Sales`) is complete — no missing values.\n",
        "- **Core identifiers** (`Name`, `Platform`, `Genre`, `Publisher`) are mostly complete, with only a few missing entries.\n",
        "- **Year_of_Release** has 269 missing values — these may need to be dropped or imputed.\n",
        "- **Review data** (`Critic_Score`, `Critic_Count`, `User_Score`, `User_Count`) is missing for about **50–55% of games**. This limits sample size for review-based hypotheses but is acceptable if we focus only on reviewed games for those analyses.\n",
        "- **Developer and Rating** have ~40% missing — these are less critical but should be noted if we use them.\n",
        "- `Name` and `Genre` each have only 2 missing entries — negligible and can be dropped.\n",
        "\n",
        "**Implications for cleaning:**\n",
        "\n",
        "- I will likely **drop rows with missing `Name` or `Global_Sales`** (key identifiers and target variable).\n",
        "- For analyses involving reviews, we’ll use the subset with non-null `Critic_Score` or `User_Score`.\n",
        "- I will consider dropping or flagging rows with missing `Year_of_Release` if time-based trends matter.\n",
        "- Missing `Developer` and `Rating` can be ignored for now since they’re not central to chosen hypotheses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check for duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see above there are no exact duplicate rows.\n",
        "\n",
        "Next I will check if there are any game titles that are duplicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df.duplicated(subset=['Name'])].sort_values(by='Name')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see here there are games that have duplicates.\n",
        "\n",
        "This is because there are games with the same name released on different consoles.\n",
        "\n",
        "These could be ports or remakes.\n",
        "\n",
        "Later in this notebook I will convert these to a dataframe where they will be combined to give the total sales of a certain game across all platforms that they are released on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remind ourselves of the null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export 1 (Base Cleaned Dataset)\n",
        "\n",
        "Before analysing the video game sales data, we need to create a cleaned and reliable version of the dataset.  \n",
        "The main goals here are to ensure that key identifiers and target variables are present, handle missing values thoughtfully, and remove columns that are not needed for the first stage of analysis.\n",
        "\n",
        "**Key cleaning steps:**\n",
        "- **Remove incomplete rows** — Dropped any records missing a `Name` or `Global_Sales` value, since these are essential identifiers and target metrics.\n",
        "- **Handle missing publishers and developers** — Replaced missing `Publisher` and `Developer` entries with `\"Unknown\"` to preserve the rows while marking incomplete data.\n",
        "- **Drop unused columns** — Removed review-related fields (`Critic_Score`, `Critic_Count`, `User_Score`, `User_Count`) and `Rating` since this first export focuses on sales and platform data only.\n",
        "- **Handle missing release years** — Replaced missing `Year_of_Release` with `-1` to keep the data but clearly mark unknown years. Converted the column to integer type for consistency.\n",
        "\n",
        "The result is a **clean, analysis-ready dataset** that focuses purely on sales, platforms, publishers, developers, and release years.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned = df.copy()\n",
        "\n",
        "df_cleaned = clean_data(df_cleaned)\n",
        "\n",
        "df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the cleaned data to a new CSV file\n",
        "df_cleaned.to_csv(os.path.join(processed_data_dir, 'video_game_sales_cleaned.csv'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# First Party Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add `is_first_party` column to the cleaned dataframe\n",
        "\n",
        "I have added it into a separate dataframe to the merged one in the next section as this one features the game sales per platform not as a whole.\n",
        "\n",
        "This way I can look at if first party games generally sell better on their own platforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a list of unique platforms\n",
        "platforms = df_cleaned['Platform'].unique().tolist() # Get unique platforms\n",
        "\n",
        "platforms.sort() # Sort the list alphabetically\n",
        "\n",
        "print(platforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_first_party = df_cleaned.copy()\n",
        "\n",
        "# define platform families (manually curated)\n",
        "nintendo = ['Wii', 'NES', 'SNES', 'GB', 'DS', '3DS', 'N64', 'GBA', 'WiiU', 'GC']\n",
        "sony = ['PS', 'PS2', 'PS3', 'PS4', 'PS5', 'PSP', 'PSV']\n",
        "microsoft = ['XB', 'X360', 'XOne', 'XSX']\n",
        "\n",
        "# row-wise masks (Publisher vs Platform family)\n",
        "mask_nin = (df_first_party['Publisher'].str.contains('nintendo', case=False, na=False)\n",
        "            & df_first_party['Platform'].isin(nintendo))\n",
        "\n",
        "mask_sony = (df_first_party['Publisher'].str.contains('sony|sce|sie|playstation', case=False, na=False)\n",
        "             & df_first_party['Platform'].isin(sony))\n",
        "\n",
        "mask_ms = (df_first_party['Publisher'].str.contains('microsoft', case=False, na=False)\n",
        "           & df_first_party['Platform'].isin(microsoft))\n",
        "\n",
        "# combine masks into one boolean column\n",
        "df_first_party['is_first_party'] = (mask_nin | mask_sony | mask_ms)\n",
        "\n",
        "df_first_party.head(25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_first_party.to_csv(os.path.join(processed_data_dir, 'video_game_sales_first_party.csv'), index=False)\n",
        "print(\"Merged dataframe saved to 'video_game_sales_first_party.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export 2 (Merged by Game Title)\n",
        "\n",
        "After creating the base cleaned dataset where each game appears once per platform, the next step was to build a **title-level dataset**.  \n",
        "This version combines all platform entries for the same game into a single row, making it easier to study overall game performance and trends without platform duplication.\n",
        "\n",
        "**Key processing steps:**\n",
        "- **Combine platform entries** — Grouped data by `Name` so each game title is represented once instead of one row per platform.\n",
        "- **Sum sales metrics** — Added together `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, and `Global_Sales` across all platforms to get total lifetime sales per game.\n",
        "- **Aggregate platforms** — Created a `Platforms` column listing all unique platforms each game was released on.\n",
        "- **Handle release year** — Selected the earliest known `Year_of_Release` for each game. If no year data was available, kept it as `Unknown`.\n",
        "- **Simplify categorical data** — For fields like `Genre`, `Publisher`, and `Developer`, kept the most frequent value across platforms. If no single value dominated, marked it as `\"Multiple\"`.\n",
        "\n",
        "The result is a **single-row-per-game dataset** that’s ideal for high-level analysis of game success, sales distribution, and market trends without platform-level duplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged = df.copy()\n",
        "\n",
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged = build_merged_df(df_merged,\n",
        "                          max_year_span=5,\n",
        "                          max_critic_diff=5.0,\n",
        "                          require_same_publisher=False)\n",
        "\n",
        "df_merged.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged = remove_review_cols(df_merged)\n",
        "\n",
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add is_multiplatform column (True/False)\n",
        "df_merged['is_multiplatform'] = df_merged['Platform'].str.contains(',').astype(bool)\n",
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged.to_csv(os.path.join(processed_data_dir, 'video_game_sales_merged.csv'), index=False)\n",
        "print(\"Merged dataframe saved to 'video_game_sales_merged.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export 3 (Cleaned with Reviews Only)\n",
        "\n",
        "The third dataset focuses specifically on **review-driven analysis**.  \n",
        "Since critic and user scores are missing for about half the games in the full dataset, this export filters down to only games with available critic reviews, ensuring reliable insights when testing review-related hypotheses.\n",
        "\n",
        "**Key processing steps:**\n",
        "- **Filter for reviewed games** — Kept only rows where `Critic_Score` is available to maintain a consistent dataset for review-based analysis.\n",
        "- **Retain review metrics** — Preserved `Critic_Score`, `Critic_Count`, `User_Score`, and `User_Count` so we can explore their relationships with global sales.\n",
        "- **Keep core sales and metadata** — Retained important fields such as `Name`, `Platform`, `Year_of_Release`, `Genre`, `Publisher`, and `Developer` to allow segmentation by platform or genre while analyzing reviews.\n",
        "- **Consistent cleaning rules** — Applied the same data cleaning steps as before (e.g., handling missing `Publisher`/`Developer` values, marking unknown years) to maintain data integrity across all exports.\n",
        "\n",
        "The result is a **focused dataset for review and ratings analysis**, ideal for testing hypotheses like *“Do higher critic scores correlate with increased global sales?”* and creating visualizations that explore the impact of reviews on game success.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exported Datasets\n",
        "\n",
        "For this project, I am creating and exporting three cleaned versions of the dataset.  \n",
        "Each serves a slightly different analytical purpose and keeps the workflow flexible.\n",
        "\n",
        "---\n",
        "\n",
        "#### `video_game_sales_clean.csv` — **Cleaned (no critic or user scores)**\n",
        "- Purpose: Base dataset for **general sales, platform, genre, and region analysis** where review data is not required.\n",
        "- Changes made:\n",
        "  - Removed rows with missing `Name` or `Global_Sales` (core identifiers and target variable).\n",
        "  - Replaced missing `Publisher` and `Developer` values with `\"Unknown\"`.\n",
        "  - Left missing `Year_of_Release` labeled as `\"Unknown\"` to keep data but mark uncertainty.\n",
        "  - Dropped critic and user review columns (`Critic_Score`, `Critic_Count`, `User_Score`, `User_Count`) to simplify analysis and dashboard builds.\n",
        "\n",
        "---\n",
        "\n",
        "#### `video_game_sales_merged_by_title.csv` — **Cleaned with multiplatform emphasis**\n",
        "- Purpose: Use for **platform-level and vendor comparisons** (e.g., Nintendo vs Sony vs Microsoft).\n",
        "- Based on the same cleaned data as above but ensures each game has all its platforms associated with it.  \n",
        "- Allows robust analysis of sales per game and region without review-related columns.\n",
        "\n",
        "---\n",
        "\n",
        "#### `video_game_sales_clean_reviews.csv` — **Cleaned with reviews only**\n",
        "- Purpose: Specific dataset for testing **hypotheses around critic scores and user scores**.\n",
        "- Created by filtering the data to include only games with a valid `Critic_Score`.\n",
        "- Keeps review-related columns (`Critic_Score`, `Critic_Count`, `User_Score`, `User_Count`) for deeper statistical analysis.\n",
        "- Useful for exploring relationships like “Do better critic scores drive higher sales?”\n",
        "\n",
        "---\n",
        "\n",
        "### Why this approach\n",
        "By splitting the cleaned data into three purpose-built CSVs:\n",
        "- Analysis and dashboards remain **lightweight** when reviews aren’t needed.\n",
        "- Review-driven insights can still be explored without dealing with ~50% missing scores in the full dataset.\n",
        "- Platform-level insights remain clean and reliable.\n",
        "\n",
        "These files are saved in the `data/processed/` folder and serve as the single sources of truth for all subsequent analysis and visualisation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create your folder here\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
